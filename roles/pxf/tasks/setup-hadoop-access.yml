# Start line to configure Hadoop access

#
- set_fact:
    java_home: "/usr/lib/jvm/jre-1.8.0"
  when: ( ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky" )

#
- set_fact:
    java_home: "/usr/lib/jvm/java-8-openjdk-amd64/jre"
  when: ( ansible_distribution == "Ubuntu" or ansible_distribution == "Debian" )

#
- name: Stop the pxf cluster start command to start PXF on each segment host
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && source /usr/local/greenplum-db/greenplum_path.sh && export JAVA_HOME={{ java_home }} && /usr/local/pxf-{{ pxf_gpdb_version }}/bin/pxf cluster stop )
  args:
    executable: /bin/bash
  register: pxf_cluster_stopped
  when:
    - inventory_hostname in groups['master']

#
- name: Print stopping pxf cluster
  debug: msg={{ pxf_cluster_stopped }}
  when:
    - inventory_hostname in groups['master']

#
- name: Create PXF base directory for Hadoop Access and query examples directory on all master and segment hosts
  become_user: gpadmin
  file: path={{ item }} state=directory mode=0755 owner=gpadmin group=gpadmin
  with_items:
    - "{{ pxf_base_dir_with_hadoop }}"
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#
- name: Prepare PXF on master for hadoop access
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && source /usr/local/greenplum-db/greenplum_path.sh && export JAVA_HOME={{ java_home }} && PXF_BASE={{ pxf_base_dir_with_hadoop }} /usr/local/pxf-{{ pxf_gpdb_version }}/bin/pxf cluster prepare )
  args:
    executable: /bin/bash
  register: pxf_cluster_hadoop_prepared
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

# https://docs.vmware.com/en/VMware-Greenplum-Platform-Extension-Framework/6.6/greenplum-platform-extension-framework/client_instcfg.html

#- name: Link source file to another destination
#  become_user: gpadmin
#  file:
#    src: "{{ pxf_base_dir_with_hadoop }}/servers/default"
#    path: "{{ pxf_base_dir_with_hadoop }}/servers/co7-master"
#    state: hard
#  when:
#    - with_hadoop_access
#    - inventory_hostname in groups['master']

#- name: Rename directory
#  become_user: gpadmin
#  shell: ( mv {{ pxf_base_dir_with_hadoop }}/servers/default {{ pxf_base_dir_with_hadoop }}/servers/default )
#  when:
#    - with_hadoop_access

#
- name: Start PXF on master for hadoop access
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && source /usr/local/greenplum-db/greenplum_path.sh && export JAVA_HOME={{ java_home }} && PXF_BASE={{ pxf_base_dir_with_hadoop }} /usr/local/pxf-{{ pxf_gpdb_version }}/bin/pxf cluster start )
  args:
    executable: /bin/bash
  register: pxf_cluster_hadoop_started
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#
- name: Initial connecton to avoid Host Verfication failed
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && sshpass -p 'changeme' ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@{{ hadoop_master_hostname }} )
  args:
    executable: /bin/bash
  register: ssh_initial_connection
  ignore_errors: true
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#- name: Create /data/pxf_examples directory in Hadoop Cluster
#  become: yes
#  become_user: gpadmin
#  shell: ( source ~/.bashrc && sshpass -p 'changeme' ssh hadoop@co7-master 'hdfs dfs -mkdir -p /data/pxf_examples' )
#  register: hdfs_directory_created
#  when:
#    - with_hadoop_access
#    - inventory_hostname in groups['master']

#
#- name: Change owner and group for /data directory in Hadoop Cluster
#  become: yes
#  become_user: gpadmin
#  shell: ( source ~/.bashrc && sshpass -p 'changeme' ssh hadoop@co7-master 'hdfs dfs -chown -R gpadmin:hadoop /data' )
#  register: hdfs_directory_permission_changed
#  when:
#    - with_hadoop_access
#    - inventory_hostname in groups['master']


# With Hadoop
#- name: Create PXF base directory for Hadoop Access with hostname of Hadoop Master
#  become_user: gpadmin
#  file: path={{ item }} state=directory mode=0755 owner=gpadmin group=gpadmin
#  with_items:
#    - "{{ pxf_base_dir_with_hadoop }}/servers/default"
#  when:
#    - with_hadoop_access
#    - inventory_hostname in groups['master']

#
- name: Copy configuration files of Hadoop
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && sshpass -p 'changeme' scp hadoop@{{ hadoop_master_hostname }}:/home/hadoop/hadoop-{{ hadoop_version }}/etc/hadoop/{{ item }} {{ pxf_base_dir_with_hadoop }}/servers/default/{{ item }} )
  args:
    executable: /bin/bash
  register: hadoop_configuratinos_copied
  ignore_errors: true
  with_items:
    - "core-site.xml"
    - "hdfs-site.xml"
    - "mapred-site.xml"
    - "yarn-site.xml"
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#
- name: Copy the pxf-profiles.xml template file to the default configuration directory.
  become_user: gpadmin
  template: src=pxf-profiles.xml.j2 dest={{ pxf_base_dir_with_hadoop}}/servers/default/pxf-profiles.xml owner=gpadmin group=gpadmin mode=644 force=yes
  when:
    - with_nfs_access
    - inventory_hostname in groups['master']

#
- name: Killall java process for PXF Server
  become: yes
  become_user: gpadmin
  shell: ( source /usr/local/greenplum-db/greenplum_path.sh && gpssh -f /home/gpadmin/hostfile -e 'killall java' )
  args:
    executable: /bin/bash
  ignore_errors: yes
  register: pxf_java_processes_killed
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#
- name: Synchronize the PXF server configuration to the GPDB cluster
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && source /usr/local/greenplum-db/greenplum_path.sh && export JAVA_HOME={{ java_home }} && PXF_BASE={{ pxf_base_dir_with_hadoop }} /usr/local/pxf-{{ pxf_gpdb_version }}/bin/pxf cluster sync )
  args:
    executable: /bin/bash
  register: pxf_cluster_hadoop_synced
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

#
- name: Start PXF server configuration to the GPDB cluster
  become: yes
  become_user: gpadmin
  shell: ( source ~/.bashrc && source /usr/local/greenplum-db/greenplum_path.sh && export JAVA_HOME={{ java_home }} && PXF_BASE={{ pxf_base_dir_with_hadoop }} /usr/local/pxf-{{ pxf_gpdb_version }}/bin/pxf cluster start )
  args:
    executable: /bin/bash
  register: pxf_cluster_hadoop_synced
  when:
    - with_hadoop_access
    - inventory_hostname in groups['master']

# Skip copy the following jar since currently hadoop distribution for mapr is not used.
# gpadmin@gpmaster$ cd $PXF_BASE/lib
# gpadmin@gpmaster$ scp mapruser@maprhost:/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/common/lib/maprfs-5.2.2-mapr.jar .
# gpadmin@gpmaster$ scp mapruser@maprhost:/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/common/lib/hadoop-auth-2.7.0-mapr-1707.jar .
# gpadmin@gpmaster$ scp mapruser@maprhost:/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/common/hadoop-common-2.7.0-mapr-1707.jar .

# testdb=# grant select on protocol pxf to gpadmin;
# testdb=# grant insert on protocol pxf to gpadmin;

# Synchronize the PXF configuration to the Greenplum Database cluster:
# gpadmin@gpmaster$ pxf cluster sync

#gpadmin@gpmaster$ cd $PXF_BASE/servers/<server_name>
#gpadmin@gpmaster$ scp hiveuser@hivehost:/etc/hive/conf/hive-site.xml .
#gpadmin@gpmaster$ pxf cluster sync

# https://stackoverflow.com/questions/17564074/accessing-files-in-hdfs-using-java
